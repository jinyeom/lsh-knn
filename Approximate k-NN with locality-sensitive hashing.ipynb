{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate k-NN with locality-sensitive hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locality-sensitive hashing (LSH)** is a method that hashes similar data points to have a high chance of being grouped together. This makes LSH a good way to optimize similarity search: by comparing the query data only with a subset of presumably similar items, you can avoid having to sort the entire dataset by some distance metric. The trade-off here is that there is no guarantee for correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is LSH exciting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generally, LSH offers a way to narrow the scope of search without any prior knowledge about the target dataset (i.e., data-independent).** This property benefits cases outside of methods like k-NN for information retrieval. For example, one such place is _inside_ of a [Transformer](https://arxiv.org/abs/1706.03762) model: the dot-product attention mechanism. Kitaev, Kaiser and Levskaya proposed an alternative architecture called [Reformer](https://arxiv.org/abs/2001.04451), which reduces the complexity of the attention mechanism from O(N^2) to O(NlogN) (where N is the number of input items) by introducing LSH to each attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this demonstration, we will generate a random \"dataset\" of Gaussian samples and a query vector from the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10000  # number of data points in the\n",
    "n = 128    # number of features in each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rng.normal(size=(m, n))  # random dataset\n",
    "q = rng.normal(size=n)       # query vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla k-NN search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we implement a vanilla k-nearest neighbors (k-NN) search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(query, data, k=5, debug=False):\n",
    "    assert k <= len(data)\n",
    "    dists = np.sqrt(np.sum((data - query) ** 2, axis=1))  # euclidean distance\n",
    "    if debug:\n",
    "        print(\"[DEBUG] max dist =\", np.max(dists))\n",
    "        print(\"[DEBUG] min dist =\", np.min(dists))\n",
    "        print(\"[DEBUG] mean dist =\", np.mean(dists))\n",
    "    inds = np.argsort(dists)  # sorted in ascending order\n",
    "    inds_k = inds[:k]         # top k closest data points\n",
    "    # NOTE: optionally, if the argumet dataset has a set of labels, we can also\n",
    "    # associate the query vector with a label (i.e., classification).\n",
    "    return data[inds_k], dists[inds_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 1: dist = 12.72876479759339\n",
      "top 2: dist = 12.980832845009397\n",
      "top 3: dist = 13.109098301375685\n",
      "top 4: dist = 13.178447300861382\n",
      "top 5: dist = 13.248307679497904\n"
     ]
    }
   ],
   "source": [
    "neighbors, dists = knn_search(q, X, debug=False)  # set debug=True for additional information\n",
    "for i, (neighbor, dist) in enumerate(zip(neighbors, dists)):\n",
    "    print(f\"top {i + 1}: dist = {dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate k-NN search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement locality-sensitive hashing (LSH) which maps a numerical vector to a hash value (an integer) with a set of random hyperplanes. The main idea of the technique is to split the input data space with a plane and determine whether the data point belongs above (1) or below (0) the plane. By repeating this technique $b$ times, each data point can be encoded to a binary string of length $b$. For the purpose of building a hash table, we convert these binary strings to decimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_hash(data, hyperplanes):\n",
    "    b = len(hyperplanes)\n",
    "    hash_key = (data @ hyperplanes.T) >= 0\n",
    "    dec_vals = np.array([2 ** i for i in range(b)], dtype=int)\n",
    "    hash_key = hash_key @ dec_vals\n",
    "    return hash_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we write a small function that generates random hyperplanes, which determines the number of hyperplanes based on a desired expected number of elements in each bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hyperplanes(data, bucket_size=16):\n",
    "    m = data.shape[0]            # number of data points\n",
    "    n = data.shape[1]            # number of features in a data point\n",
    "    b = m // bucket_size         # desired number of hash buckets\n",
    "    h = int(np.log2(b))          # desired number of hyperplanes\n",
    "    H = rng.normal(size=(h, n))  # hyperplanes as their normal vectors\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locality_sensitive_hash(data, hyperplanes):\n",
    "    hash_vals = hamming_hash(data, hyperplanes)\n",
    "    hash_table = {}\n",
    "    for i, v in enumerate(hash_vals):\n",
    "        if v not in hash_table:\n",
    "            hash_table[v] = set()\n",
    "        hash_table[v].add(i)\n",
    "    return hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_bucket_size = 19.53125\n"
     ]
    }
   ],
   "source": [
    "hyperplanes = generate_hyperplanes(X)\n",
    "hash_table = locality_sensitive_hash(X, hyperplanes)\n",
    "avg_bucket_size = np.mean([len(v) for v in hash_table.values()])\n",
    "print(\"avg_bucket_size =\", avg_bucket_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement a k-NN search algorithm that incorporates LSH. Note that we can repeat the search with more than one set of hyperplanes to boost the accuracy. Feel free to experiment by tweaking the argument `repeat=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_knn_search(query, data, k=5, bucket_size=16, repeat=10, debug=False):\n",
    "    candidates = set()\n",
    "    for i in range(repeat):\n",
    "        hyperplanes = generate_hyperplanes(data)\n",
    "        hash_table = locality_sensitive_hash(data, hyperplanes)\n",
    "        if debug:\n",
    "            avg_bucket_size = np.mean([len(v) for v in hash_table.values()])\n",
    "            print(f\"[DEBUG] i = {i}, avg_bucket_size = {avg_bucket_size}\")\n",
    "        query_hash = hamming_hash(query, hyperplanes)\n",
    "        if query_hash in hash_table:\n",
    "            candidates = candidates.union(hash_table[query_hash])\n",
    "    candidates = np.stack([data[i] for i in candidates], axis=0)\n",
    "    return knn_search(query, candidates, k=k, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 1: dist = 12.980832845009397\n",
      "top 2: dist = 13.315716707872218\n",
      "top 3: dist = 13.599262272317079\n",
      "top 4: dist = 13.77326305995105\n",
      "top 5: dist = 13.810928200015331\n"
     ]
    }
   ],
   "source": [
    "neighbors, dists = approx_knn_search(q, X, repeat=24, debug=False)  # set debug=True for additional information\n",
    "for i, (neighbor, dist) in enumerate(zip(neighbors, dists)):\n",
    "    print(f\"top {i + 1}: dist = {dist}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
